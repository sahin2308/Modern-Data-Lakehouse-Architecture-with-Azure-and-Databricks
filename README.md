# ğŸš€ Modern Data Lakehouse Architecture using Azure & Databricks

An end-to-end Data Engineering solution built using **Azure Data Factory**, **Azure Data Lake Storage Gen2**, **Azure Databricks**, and **Unity Catalog**. This project demonstrates how to design and implement a modern **Lakehouse architecture** using best practices for **data ingestion**, **processing**, **orchestration**, **incremental loading**, and **dimensional modeling**.

---

## ğŸ“Œ Project Overview

This project simulates a real-world pipeline where:

- Raw files are ingested using **ADF** and stored in **ADLS Gen2**.
- **Databricks Autoloader** incrementally loads data to the **Bronze Layer**.
- Transformation logic is applied in the **Silver Layer** using **PySpark**.
- **Star Schema** (Dimension + Fact tables) is created in the **Gold Layer**.
- **SCD Type 1 & Type 2** logic is implemented using PySpark and **Delta Live Tables**.
- The entire workflow is **orchestrated using Databricks Jobs**.

---

## ğŸ› ï¸ Tech Stack

| Tool/Service                | Purpose                                      |
|----------------------------|----------------------------------------------|
| Azure Data Factory (ADF)   | Data ingestion from source to ADLS Gen2      |
| ADLS Gen2                  | Raw, Bronze, Silver, and Gold data storage   |
| Azure Databricks           | Data transformation, orchestration, modeling |
| Unity Catalog              | Secure cataloging of Delta tables            |
| Delta Lake                 | Storage format for all structured layers     |
| Delta Live Tables (DLT)    | Streaming & SCD Type 2 implementation        |
| PySpark                    | Data engineering transformations             |
| Databricks Jobs            | Orchestration across pipeline layers         |

---

## ğŸ—‚ï¸ Folder Structure

```bash
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Bronze_Layer.ipynb     # Incremental load using Autoloader
â”‚   â”œâ”€â”€ Parameter.ipynb
â”‚   â”œâ”€â”€ Silver_Customers.ipynb # Cleansing, deduplication, logic
â”‚   â”œâ”€â”€ Silver_Orders.py
â”‚   â”œâ”€â”€ Silver_Products.ipynb
â”‚   â”œâ”€â”€ Silver_Regions.ipynb
â”‚   â”œâ”€â”€ Gold_Customers.ipynb        # Dimension and Fact tables (SCD1)
â”‚   â”œâ”€â”€ Gold_Products.ipynb
â”‚   â”œâ”€â”€ Gold_Orders.ipynb
â”‚   â””â”€â”€ scd_dimensional_dlt.py            # DLT pipeline for SCD Type 2
â”‚   â””â”€â”€ Databricks ETE Project.dbc

â”‚
â”œâ”€â”€ orchestrated/
â”‚   â””â”€â”€ dlt_pipeline_yaml_code        # ADF pipeline export
â”‚   â””â”€â”€ Orchestrated Job Python Code

â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample-files/                     # Sample Parquet files
â”‚       â”œâ”€â”€ customer_first.parquet
â”‚       â””â”€â”€ customers_second.parquet
â”‚       â””â”€â”€ orders_first.parquet
â”‚       â””â”€â”€ orders_second.parquet
â”‚       â””â”€â”€ products_first.parquet
â”‚       â””â”€â”€ products_second.parquet
â”‚       â””â”€â”€ regions.parquet
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ architecture-diagram.png
â”‚   â”œâ”€â”€ adf-pipeline.png
â”‚   â”œâ”€â”€ orchestrated-job-screenshot.png
â”‚   â””â”€â”€ dlt-pipeline-screenshot.png
â”‚   â””â”€â”€ unity-data-catalog-screenshot.png
â”‚
â””â”€â”€ README.md
